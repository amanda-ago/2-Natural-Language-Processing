---
title: "Natural Language Processing"
author: "Amanda Oliveira"
---

```{r}
# install.packages("tm")
# install.packages("SnowballC")
# install.packages("wordcloud")
# install.packages("ggplot2")
# install.packages("tidyverse") 
# install.packages("topicmodels")
# install.packages("readxl")
# install.packages("pdftools")
# install.packages("ggjoy")

library(tm)
library(SnowballC)
library(wordcloud)
library(ggplot2)
library(tidyverse) 
library(topicmodels)
library(readxl)
library(pdftools)
library(ggjoy)

```

## **1. Application: Classroom Notes** 

#### **1.1. Data Wrangling**

```{r}

# Student notes are saved as weekly csv files. These are real notes taken by students enrolled in the HUDK4050 class during the Fall/2020.

#Create a list of all the files, then loop over file list importing them and binding them together
getwd()
D1 <- list.files(path = "./data/class-notes/",
               pattern = "*.csv", 
               full.names = T) %>% 
    map_df(~read_csv(., col_types = cols(.default = "c"))) 
  
#Separate out the variables of interest
D1 <- select(D1, Title, Notes)

#Remove the htlm tags from your text
D1$Notes <- gsub("<.*?>", "", D1$Notes)
D1$Notes <- gsub("nbsp", "" , D1$Notes)
D1$Notes <- gsub("nbspnbspnbsp", "" , D1$Notes)
D1$Notes <- gsub("<U+00A0><U+00A0><U+00A0>", "" , D1$Notes)

#Merge the weeks data with your notes data so that each line has a week attributed to it 
D2 <- read.csv("./data/week-list.csv")
D1 <- left_join(D1, D2)

#Also remove readings not belonging to the class (IE - that are NA for week)
D1 <- D1 %>% drop_na(week)


```

#### **1.2. Process text using the tm package**

```{r}
#Convert the data frame to the corpus format that the tm package uses
corpus <- VCorpus(VectorSource(D1$Notes))
#Remove spaces
corpus <- tm_map(corpus, stripWhitespace)
#Convert to lower case
corpus <- tm_map(corpus, tolower)
#Remove pre-defined stop words ('the', 'a', etc)
corpus <- tm_map(corpus, removeWords, stopwords('english'))
#Convert words to stems ("education" = "edu") for analysis, for more info see  http://tartarus.org/~martin/PorterStemmer/
corpus <- tm_map(corpus, stemDocument)
#stemCompletion() - check it out!
#Remove numbers
corpus <- tm_map(corpus, removeNumbers)
#remove punctuation
corpus <- tm_map(corpus, removePunctuation)
#Convert to plain text for mapping by wordcloud package
corpus <- tm_map(corpus, PlainTextDocument, lazy = TRUE)

#Convert corpus to a term document matrix - so each word can be analyzed individually
tdm.corpus <- TermDocumentMatrix(corpus)

#Note: we won't remove plural words here, plural words in English tend to be highly irregular and difficult to extract reliably

# Steps explained: The first step here was a tokenization: All sentences in a body of text are split into words - so that each word counts as a single "data point". Next we needed remove "duplicate" and "meaningless" words, as we wanted generate a matrix containing only the words we wanted to analyze. In our example, numbers, punctuation, and stop words should not be counted. Recasting variables to lower case avoids that the words "Education" and "education" are counted separately. Stemming makes sure that only the "root" of the word is counted (so "educator" and "education" are both reduced to "educ").
 
```

#### **1.3. Find Common Words**

```{r}
#The tm package can do some simple analysis, like find the most common words
findFreqTerms(tdm.corpus, lowfreq=500, highfreq=Inf)
#We can also create a vector of the word frequencies that can be useful to see common and uncommon words
word.count <- sort(rowSums(as.matrix(tdm.corpus)), decreasing=TRUE)
word.count <- data.frame(word.count)
#Look at the word.count dataframe

summary(word.count)

```
#### **1.4. Generate a Word Cloud**

```{r}
#Define the colors the cloud will use
col=brewer.pal(6,"Dark2")
#Generate cloud, make sure your window is large enough to see it
set.seed(97)
wordcloud(corpus, min.freq=500, scale=c(5,2),rot.per = 0.25,
          random.color=T, max.word=45, random.order=F,colors=col)
```

#### **1.5. Sentiment Analysis**

```{r}
### Match words in corpus to lexicons of positive & negative words

#Upload positive and negative word lexicons
positive <- readLines("./data/positive-words.txt")
negative <- readLines("./data/negative-words.txt")

#Search for matches between each word and the two lexicons
D1$positive <- tm_term_score(tdm.corpus, positive)
D1$negative <- tm_term_score(tdm.corpus, negative)

#Generate an overall pos-neg score for each line
D1$score <- D1$positive - D1$negative

```
#### **1.6. ggplot Visualization**

```{r}
#visualization of the mean sentiment score over weeks, removing rows that have readings from other classes (NA for weeks):

summary(D1$score)

ggplot(D1,aes(x = score, y = as.factor(week), fill=week)) +
    geom_joy(scale = 2, fill.color="darkred") + theme_joy() + theme_minimal() + 
    theme(legend.position = "none") +
    labs(title = "Sentiment Score over Weeks",
         x = "Sentiment Score",
         y = "Week") 

ggplot(D1, aes(x=as.factor(week), y=score)) +
    geom_point(shape=1) +
    labs(title = "Weekly Sentiment Scores",
         x = "Week",
         y = "Sentiment Score") 

```

#### **1.7. LDA Topic Modelling**

```{r}

## Using the same csv file you have generated the LDA analysis will treat each row of the data frame as a document. In fact, each individual class note will be treated as an independent document. Then we will attempt to group the notes by topic using LDA.

#Term Frequency Inverse Document Frequency
dtm.tfi <- DocumentTermMatrix(corpus, control = list(weighting = weightTf))

#Remove very uncommon terms (term freq inverse document freq < 0.1)
dtm.tfi <- dtm.tfi[,dtm.tfi$v >= 0.1]

#Remove non-zero entries
rowTotals <- apply(dtm.tfi , 1, sum) #Find the sum of words in each Document
dtm.tfi2   <- dtm.tfi[rowTotals> 0, ] #Divide by sum across rows

#Identify rows with zero entries
which(rowTotals %in% c(0))

#Remove these rows from original dataset
D3 <- D1[-c(which(rowTotals %in% c(0))),]

#Generate LDA model, k is the number of topics and the seed is a random number to start the process
lda.model = LDA(dtm.tfi2, k = 5, seed = 150)

#Which terms are most common in each topic
terms(lda.model, k = 10) 

#Identify which documents belong to which topics based on the notes taken by the student
D3$topic <- topics(lda.model)

#What does an LDA topic represent? An LDA topic is an abstract "topic" derived from the word distributions in a set of documents. In LDA each document belongs to a single topic - and multiple documents are grouped together based on the words they contain.

```

## **2. Application: NYT Articles (pdf format)** 

#### **2.1. Import PDFs to a dataframe**

```{r}

# Note: I selected 12 New York Times Articles and saved them as pdfs.
# The articles were picked from three sections of the website: Arts, Sports, and Science
# The specific article theme varies widely. See below: 

Articles <- read_excel("./data/pdf_articles/Articles.xlsx")

out.file <- ""
files <- list.files(path="./data/pdf_articles/", pattern="*.pdf", full.names=TRUE, recursive=FALSE)
for (file in files) {
  pdf <- pdf_text(file)
  pdf <- as.data.frame(pdf)
  pdf <- mutate(pdf, x=1)
  pdf <- pdf %>% 
  group_by(x) %>%
  mutate(allpages = paste(pdf, collapse = " | ")) %>% select(allpages) %>% unique() %>% subset(, select = c(allpages)) %>% mutate(File=file)
  out.file <- rbind(out.file, pdf)
}

out.file$File <- str_remove_all(out.file$File, "C:/HUDK4051/Project3_NLP/natural-language-processing/pdf_articles/")
out.file$File <- str_remove_all(out.file$File, ".pdf")

out.file <- out.file[-c(1), ]

```

#### **2.2. Process Data: Corpus**

```{r}

corpus2 <- VCorpus(VectorSource(out.file$allpages)) # convert to corpus format
corpus2 <- tm_map(corpus2, stripWhitespace) #remove spaces
corpus2 <- tm_map(corpus2, content_transformer(tolower)) # lower case
#corpus2 <- tm_map(corpus2, tolower, lazy=TRUE) # lower case
corpus2 <- tm_map(corpus2, removeWords, stopwords('english')) # remove stop words (Not removing "the"?????)
corpus2 <- tm_map(corpus2, stemDocument) #stem
corpus2 <- tm_map(corpus2, removeNumbers) # remove numbers
corpus2 <- tm_map(corpus2, removePunctuation) # remove punctuation 

```
#### **2.3. LDA Topic Modeling**

```{r}

dtm.tfi <- DocumentTermMatrix(corpus2, control = list(weighting = weightTf)) #Term Frequency Inverse Document Frequency
dtm.tfi <- dtm.tfi[,dtm.tfi$v >= 0.1] #Remove very uncommon terms
rowTotals <- apply(dtm.tfi , 1, sum) #Find the sum of words in each Document
rowTotals
dtm.tfi2   <- dtm.tfi[rowTotals> 0, ] #Divide by sum across rows
which(rowTotals %in% c(0)) #Identify rows with zero entries: none
lda.model = LDA(dtm.tfi2, k = 3, seed = 150) #Generate LDA model. I know I want three topics
terms(lda.model, k = 10) # Common terms by topic

```
#### **2.4. See results**

```{r}

out.file$topic <- topics(lda.model) #Identify document/topic relationship

out.file[,c(2,3)]

# Note: The algorithm did not identify the "topics" successfully. 
# That was certainly due to a very small & noisy sample of articles.
# Increasing the sample and/or making the article themes more uniform would certainly make the results "better".

```









